
# Artigo 1

> HPC@Cloud: A Provider-Agnostic Software Framework forEnabling HPC in Public Cloud Platforms

Most of the research and industry efforts regarding HPC and cloud computing is focused on understanding the cost-benefit of moving legacy applications from on-premise clusters to a public cloud platform. Research in the field is also focused on how to extract the best performance possible from unknown underlying hardware, connected through a relatively much slower and unreliable network [Netto et al. 2018].

Somasundaram et al.followed a similar approach that of our study, proposing and testing a resource broker for minimizing execution time and costs of jobs in HPCClouds [Somasundaram and Govindarajan 2014]. Their solution is based on a ParticleSwarm Optimization (PSO) allocation mechanism.Pe ̃na-Monferrer et al.proposed acloud native framework for executing Computational Fluid Dynamics (CFD) workloads based on an hybrid cloud architecture [Pe ̃na-Monferrer et al. 2021]. The majority of the computational load is executed in a traditional on-premises HPC cluster, while the data processing of the results is done using public cloud infrastructure. Our research has a different scope and infrastructure, since we focus on bringing HPC workloads entirely to the public cloud. Another framework was proposed byWong et al., which focused on the deployment and exposure of HPC applications as SaaS offerings in public clouds [Wong and Goscinski 2013]. Our research differs as it does not tackle the issue of servicing HPC applications but the feasibility of executing them using public cloud infrastructure. Although the researchers addressed the issues of configuring cloud resources forHPC, they do not analyzed the cost-effectiveness of actually executing these workloads in the cloud.

Recent works have also been studying the cost-effectiveness and reliability of spot machines.Qu et al.discussed both aspects when using AWS spot in-stances [Qu et al. 2016]. However, they focused on enterprise web applications and notHPC workloads, being a different context than the one considered in our work. Teylo et al.presented an extensive evaluation of AWS spot machines for scheduling bag-of-tasks applications [Teylo et al. 2021], which we consider complimentary to our research.However, our work goes further, bringing a generalized toolset for migrating not only bag-of-tasks applications but tightly-coupled HPC workloads as well.Li et al.presented a comparative bibliographical investigation on the pros and cons of the spot and fixed-pricing schemes for cloud computing [Li et al. 2015], whereas our study presents a software framework proposal and practical analysis of how to actually migrate MPI applications to the cloud.

Finally, there have been some efforts on providing fault tolerance for applications running on cloud infrastructures.Gong et al.andVoorsluys et al.investigated the efficiency of fault tolerance techniques based on BLCR using AWS infrastructure, analyzing how replicated execution can help on reducing monetary costs [Gong et al. 2015,Voorsluys and Buyya 2012]. We consider our study complimentary, as our proposed framework can be extended to support not only BLCR, but other fault tolerance mechanisms as well. Our research also has a larger scope, focusing on building tools to test and predict job execution costs. Egwutuoha et. al proposed a fault tolerance framework based on Process Level Redundancy (PLR) for executing HPC applications in the Cloud. Our research differs in scope and also considers different fault tolerance mechanisms, such as system-level checkpoint restart.Yi et al.presented an analysis of spot AWS resources cost-reductions granted by different checkpointing policies [Yi et al. 2010]. Differently, in this paper, we neither focused on evaluating checkpointing policies nor limited our contributions to a specific cloud provider and pricing model.

# Artigo 2

>Design Space Exploration of Heterogeneous Systems Applied to the Cloud Resource Allocation Problem

Cloud providers like AWS [Amazon 2020], Google Cloud1, and Microsoft Azure2pro-vide to their users a diversity of virtual machine types and cloud configurations to be instantiated. There are different costs and performance depending on the chosen VM instance so that it is hard to a user to choose the best VM configuration to meet its application demands. The task of choosing a suitable VM configuration to the applications is subject to the Cloud Resource Allocation Problem and it is also a current topic of research in the literature.

There are proposals that seek to solve the cloud resource allocation problem by extracting information from multiple application runs to model the resources and performance demands of applications [Yadwadkar et al. 2017][Venkataraman et al. 2016]. The collected information constitutes a training set for machine learning algorithms aimed at recommending cloud configurations. Ernest and PARIS are examples of systems that have gone into this direction.

Ernest [Venkataraman et al. 2016] exploits the internal structure of a workload to generate a linear model that predicts execution time. The prediction needs only a small amount of data as input, significantly reducing the cost of measurement.PARIS [Yadwadkar et al. 2017], on the other hand, builds a measurement-based RandomForest model to make predictions of execution time or operation cost of workloads.Some authors focused on exploring the search space of instance configurations.Techniques such as random search and network search achieved lower performancewhen compared to statistical methods such as Bayesian Optimization with Random Forest [Hsu et al. 2018][Rosario et al. 2020] or Gausian Process [Alipourfard et al. 2017].Those approaches have the benefit of low-cost search by looking at a few configurations.In [Rosario et al. 2020] the search cost is even lower with the adoption of ParamountInteractions.The proposal presented in this paper differs from the research work found inthe literature by not resorting to the cloud provider at any stage of the working flow.MultiExplorer-VM, on the other hand, adopts the a simulation tool [Calheiros et al. 2011]to obtain the performance and cost data that will act as inputs (constraints) to the DSE module. The DSE module uses time and cost accurate predictors to estimate runtime and cost of the applications running on the proposed cloud resource. Another highlight of this work is the ability to search for VM instances that meet elasticity and heterogeneous VM configurations. Such features can lead to better cost×runtime relationships for determining the configuration of VM instances suitable to the workload.
# Artigo X
>REZENDE, João Ladeira; SANTOS, Edevaldo Braga dos; CAVALHEIRO, Gerson Geraldo H.. Detecção de operações de redução em programas C. _In_: SIMPÓSIO EM SISTEMAS COMPUTACIONAIS DE ALTO DESEMPENHO (WSCAD), 22. , 2021, Belo Horizonte. **Anais** [...]. Porto Alegre: Sociedade Brasileira de Computação, 2021 . p. 204-215. DOI: [https://doi.org/10.5753/wscad.2021.18524](https://doi.org/10.5753/wscad.2021.18524).

...
### Finalizando a seção
Na sequência deste texto, o problema de detectar reduções não identificadas nas ferramentas estudadas é objeto de estudo. É apresentada e discutida uma estratégia estática de detecção de reduções para identificação de laços contendo padrões de redução que sejam candidatos à paralelização, mesmo que necessitando intervenção do programador para obtenção desta paralelização.

